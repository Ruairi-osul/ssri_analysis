{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import community as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\".\").absolute().parent / \"data\"\n",
    "fig_dir = Path(\".\").absolute().parent / \"figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(data_dir / \"rsctest_LC_sizes.csv\")\n",
    "    .loc[lambda x: x.binsize == 0.2]\n",
    "    .assign(r= lambda x: x.R_spike_count)\n",
    "    .loc[:, [\"spiketrain_1\", \"spiketrain_2\", \"r\", \"p\", \"spiketrain_1_cluster\", \"spiketrain_2_cluster\", \"session_name\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(data_dir / \"evoked_rsc.csv\")\n",
    "    .loc[:, [\"spiketrain_1\", \"spiketrain_2\", \"r\", \"p\", \"session_name\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(\n",
    "    df: pd.core.frame.DataFrame, \n",
    "    st1_col: str = \"spiketrain_1\",\n",
    "    st2_col: str = \"spiketrain_2\",\n",
    "    corr_col: str = \"r\",\n",
    "    corr_rank_method: str = \"mag\",\n",
    "    degree_threshold_method: str = \"log_n_nodes\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Construct a graph for a single recording session.\n",
    "    \n",
    "    A node is created for each neuron. An edge is drawn between neurons\n",
    "    with high correlation until the mean network degree reaches a threshold value.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas dataframe containing one row for each correlated neuron pair.\n",
    "            Must only contain data from a single recording session.\n",
    "        st1_col: The name of the column in df containing the first neuron in the correlation\n",
    "        st2_col: The name of the column in df containing the second neuron in the correlation\n",
    "        corr_col: The name of the column in df containing the correlation coeficient\n",
    "        corr_rank_method: Method to use when ranking correlations {\"mag\", \"identity\"}. \n",
    "                          'mag'      - ranks correlations by their magnitude,\n",
    "                          'identity' - ranks correlations by their real value.\n",
    "                          'min'      - lowest to highest\n",
    "        degree_threshold_method: Method to use when calculating the threshold for mean degree\n",
    "                                 centrality. {\"log_n_nodes\"}\n",
    "                                 'log_n_nodes' - log10 of the number of nodes in the network\n",
    "                                 'all' - Connect each correlation\n",
    "    Returns:\n",
    "        A networkx undirected Graph object\n",
    "    \"\"\"\n",
    "    # get set of nodes -> neuron_ids\n",
    "    st1, st2 = df[st1_col].unique(), df[st2_col].unique()\n",
    "    neuron_ids = np.union1d(df.spiketrain_1.unique(), df.spiketrain_1.unique())\n",
    "    \n",
    "    # construct an empty graph and initialise the nodes\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(neuron_ids)\n",
    "    \n",
    "    # parse correlation ranking method\n",
    "    if corr_rank_method == \"mag\":\n",
    "        df = df.assign(corr_mag=lambda x: np.abs(x[corr_col]))\n",
    "        corr_col = \"corr_mag\"\n",
    "    elif corr_rank_method == \"identity\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Correlation Ranking Method: {corr_rank_method}\")\n",
    "    \n",
    "    # parse degree threshold\n",
    "    if degree_threshold_method == \"log_n_nodes\":\n",
    "        thresh = np.log(len(nx.nodes(G)))\n",
    "    elif degree_threshold_method == \"all\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Degree Threshold: {degree_threshold_method}\")\n",
    "    \n",
    "    # draw edges between nodes until threshold has been met\n",
    "    for row in df.sort_values(corr_col).to_dict(orient=\"row\"):\n",
    "        n1, n2 = row[st1_col], row[st2_col]\n",
    "        \n",
    "        G.add_edge(n1, n2)\n",
    "        \n",
    "        if degree_threshold_method != \"all\":\n",
    "            mean_degree = np.mean(list(dict(G.degree).values()))\n",
    "            if mean_degree >= thresh:\n",
    "                G.remove_edge(n1, n2)\n",
    "                break\n",
    "                \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contruct_surrogate_graphs(\n",
    "    df: pd.core.frame.DataFrame,\n",
    "    n:int = 100,\n",
    "    st1_col: str = \"spiketrain_1\",\n",
    "    st2_col: str = \"spiketrain_2\",\n",
    "    corr_col: str = \"r\",\n",
    "    corr_rank_method: str = \"mag\",\n",
    "    degree_threshold_method: str = \"log_n_nodes\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Construct surrogate graphs by permuting correlations.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas dataframe containing one row for each correlated neuron pair.\n",
    "            Must only contain data from a single recording session.\n",
    "        st1_col: The name of the column in df containing the first neuron in the correlation\n",
    "        st2_col: The name of the column in df containing the second neuron in the correlation\n",
    "        corr_col: The name of the column in df containing the correlation coeficient\n",
    "        corr_rank_method: Method to use when ranking correlations {\"mag\", \"identity\"}. \n",
    "                          'mag'      - ranks correlations by their magnitude,\n",
    "                          'identity' - ranks correlations by their real value.\n",
    "        degree_threshold_method: Method to use when calculating the threshold for mean degree\n",
    "                                 centrality. {\"log_n_nodes\"}\n",
    "                                 'log_n_nodes' - log10 of the number of nodes in the network\n",
    "    Returns:\n",
    "        A networkx undirected Graph object\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    for i in range(n):\n",
    "        graphs.append(\n",
    "            df.assign(st1_perm=lambda x: np.random.permutation(x[st1_col]),\n",
    "                      st2_perm=lambda x: np.random.permutation(x[st2_col]))\n",
    "            .pipe(lambda x: create_graph(\n",
    "                            df=x, st1_col=\"st1_perm\", st2_col=\"st2_perm\", \n",
    "                            corr_col=corr_col, corr_rank_method=corr_rank_method,\n",
    "                            degree_threshold_method=degree_threshold_method\n",
    "                           )\n",
    "                 )\n",
    "        )\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_modularity(G, n=500):\n",
    "    n_nodes = len(G.nodes())\n",
    "    n_edges = len(G.edges())\n",
    "    s_graphs = [nx.generators.random_graphs.gnm_random_graph(\n",
    "                    n=n_nodes, \n",
    "                    m=n_edges,\n",
    "                ) for i in range(n)]\n",
    "    mods = np.array(list(map(get_modularity, s_graphs)))\n",
    "    mod = get_modularity(G)\n",
    "    return np.mean(mods >= mod), mods.mean(), mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_ids(df):\n",
    "    return np.union1d(df.spiketrain_1.unique(), df.spiketrain_1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modularity(G):\n",
    "    partition = c.best_partition(G)\n",
    "    return c.modularity(partition, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.545454545454546%\n",
      "5.545454545454546%\n",
      "6.545454545454546%\n",
      "7.545454545454546%\n",
      "8.545454545454547%\n",
      "9.545454545454547%\n",
      "10.545454545454547%\n",
      "11.545454545454547%\n",
      "12.545454545454547%\n",
      "13.545454545454547%\n",
      "14.545454545454547%\n",
      "15.545454545454547%\n",
      "16.545454545454547%\n",
      "17.545454545454547%\n",
      "18.545454545454547%\n",
      "19.545454545454547%\n",
      "20.545454545454547%\n",
      "21.545454545454547%\n",
      "22.545454545454547%\n",
      "23.545454545454547%\n",
      "24.545454545454547%\n",
      "25.545454545454547%\n"
     ]
    }
   ],
   "source": [
    "sessions = df[\"session_name\"].unique()\n",
    "d = []\n",
    "for i, session in enumerate(sessions):\n",
    "    print(f\"{(i + 1)/len(sessions) * 100}%\")\n",
    "    df1 = (\n",
    "        df\n",
    "        .loc[lambda x: x.session_name == session]\n",
    "    )\n",
    "    G = create_graph(df1, degree_threshold_method=\"log_n_nodes\")\n",
    "    p, m, ob = test_modularity(G, n=1000)\n",
    "    d.append({\"session\": session, \"p\": p, \"mean\": m, \"obs\": ob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>p</th>\n",
       "      <th>mean</th>\n",
       "      <th>obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ESHOCK_03_LOC1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.194278</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESHOCK_04_LOC1</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.392902</td>\n",
       "      <td>0.314694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESHOCK_06_LOC1</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.389787</td>\n",
       "      <td>0.318939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESHOCK_07_LOC1</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.330714</td>\n",
       "      <td>0.160714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESHOCK_08_LOC1</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.409950</td>\n",
       "      <td>0.375857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ESHOCK_09_LOC1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.415656</td>\n",
       "      <td>0.330688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hamilton_10</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.247403</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hamilton_03</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.295259</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hamilton_04</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.350518</td>\n",
       "      <td>0.293750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hamilton_09</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.336670</td>\n",
       "      <td>0.201172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hamilton_31</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.425593</td>\n",
       "      <td>0.344783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hamilton_38</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.405997</td>\n",
       "      <td>0.372634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hamilton_37</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.415879</td>\n",
       "      <td>0.371809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hamilton_35</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.425461</td>\n",
       "      <td>0.318130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hamilton_36</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.397732</td>\n",
       "      <td>0.343316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hamilton_32</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.404809</td>\n",
       "      <td>0.372904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>acute_15</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.409471</td>\n",
       "      <td>0.425481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>acute_16</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.365264</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>acute_01</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.377476</td>\n",
       "      <td>0.286694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>acute_14</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.378403</td>\n",
       "      <td>0.322359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>acute_12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.414930</td>\n",
       "      <td>0.323462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>acute_11</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.410647</td>\n",
       "      <td>0.363804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           session      p      mean       obs\n",
       "0   ESHOCK_03_LOC1  1.000  0.194278  0.000000\n",
       "1   ESHOCK_04_LOC1  0.987  0.392902  0.314694\n",
       "2   ESHOCK_06_LOC1  0.963  0.389787  0.318939\n",
       "3   ESHOCK_07_LOC1  0.998  0.330714  0.160714\n",
       "4   ESHOCK_08_LOC1  0.889  0.409950  0.375857\n",
       "5   ESHOCK_09_LOC1  1.000  0.415656  0.330688\n",
       "6      hamilton_10  0.969  0.247403  0.111111\n",
       "7      hamilton_03  0.845  0.295259  0.222222\n",
       "8      hamilton_04  0.884  0.350518  0.293750\n",
       "9      hamilton_09  0.996  0.336670  0.201172\n",
       "10     hamilton_31  1.000  0.425593  0.344783\n",
       "11     hamilton_38  0.849  0.405997  0.372634\n",
       "12     hamilton_37  0.958  0.415879  0.371809\n",
       "13     hamilton_35  1.000  0.425461  0.318130\n",
       "14     hamilton_36  0.944  0.397732  0.343316\n",
       "15     hamilton_32  0.848  0.404809  0.372904\n",
       "16        acute_15  0.279  0.409471  0.425481\n",
       "17        acute_16  0.997  0.365264  0.216553\n",
       "18        acute_01  0.988  0.377476  0.286694\n",
       "19        acute_14  0.917  0.378403  0.322359\n",
       "20        acute_12  1.000  0.414930  0.323462\n",
       "21        acute_11  0.957  0.410647  0.363804"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.3443251150558843, 0.4188034188034188)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_modularity(G, n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
